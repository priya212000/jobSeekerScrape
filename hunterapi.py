# -*- coding: utf-8 -*-
"""hunterApi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10oA8Y1fbuek0nBPQ5rYs-0sneq0s18N2
"""

import scrapy
import json
import pandas as pd
from scrapy.crawler import CrawlerProcess

class MySpider(scrapy.Spider):
    name = "myspider"

    def start_requests(self):
        # Load the CSV file into a pandas DataFrame
        df = pd.read_csv('test.csv')

        # Loop over each row in the DataFrame and construct the API endpoint URL
        for index, row in df.iterrows():
            company = row['company']
            full_name = row['name']
            first_name, last_name = full_name.split()
            url = f'https://api.hunter.io/v2/domain-search?company={company}&api_key=f0b0d9e5a2845ab152e53cb61009351b429fdb32'
            
            yield scrapy.Request(url=url, callback=self.companyParse, meta={'index': index, 'df': df})
            

    def companyParse(self, response):
        data = json.loads(response.text)
        domain = data['data']['domain']
        index = response.meta['index']
        df = response.meta['df']
        df.at[index, 'domain'] = domain
        df.to_csv('test.csv', index=False)

        # Extract the email address using the updated domain column
        company = df.loc[index, 'domain']
        full_name = df.loc[index, 'name']
        first_name, last_name = full_name.split()
        url = f'https://api.hunter.io/v2/email-finder?domain={company}&first_name={first_name}&last_name={last_name}&api_key=f0b0d9e5a2845ab152e53cb61009351b429fdb32'

        yield scrapy.Request(url=url, callback=self.parse, meta={'index': index, 'df': df})

    def parse(self, response):
        # Parse the JSON response and extract the email address
        data = json.loads(response.text)
        email = data['data']['email']

        # Update the current row with the extracted email address
        index = response.meta['index']
        df = response.meta['df']
        df.at[index, 'email'] = email

        # Write the updated DataFrame back to the CSV file
        df.to_csv('output.csv', index=False)

process = CrawlerProcess()
process.crawl(MySpider)
process.start()